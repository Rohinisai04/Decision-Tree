import pandas as pd
import numpy as np

# Load the data
dataset = pd.read_csv('EnjoySports.csv')
dataset.head()
dataset.shape

# Function to calculate entropy
def calc_entropy(dataset):
    target_col = dataset.iloc[:, -1]
    class_counts = target_col.value_counts()
    entropy = 0
    for val in class_counts.keys():
        prob = class_counts[val] / sum(class_counts)
        entropy -= prob * np.log2(prob)
    return entropy

# Function to calculate information gain (ID3)
def info_gain(dataset, feature):
    entropy_set = calc_entropy(dataset)
    unique_vals = dataset[feature].unique()
    feature_counts = dataset[feature].value_counts()
    entropy_feature_vals = []

    for val in range(len(unique_vals)):
        subset = dataset[dataset[feature] == unique_vals[val]]
        subset_entropy = 0
        for outcome in dataset.iloc[:, -1].unique():
            try:
                prob_outcome = subset.iloc[:, -1].value_counts()[outcome] / len(subset)
                subset_entropy -= prob_outcome * np.log2(prob_outcome)
            except:
                subset_entropy = 0
        entropy_feature_vals.append(subset_entropy)

    for i in range(len(unique_vals)):
        entropy_set -= feature_counts[unique_vals[i]] * entropy_feature_vals[i] / sum(feature_counts)

    return entropy_set

# Class definition for a Node in the decision tree
class TreeNode():
    def __init__(self, feature=None, possible_values=None):
        self.feature = feature
        self.possible_values = possible_values

    def get_name(self):
        return self.feature

# Function to select the best attribute based on Information Gain (ID3)
def select_best_feature(dataset, used_features):
    node = TreeNode()
    max_info_gain = 0
    best_feature = None
    candidate_features = [feature for feature in dataset.columns[:-1] if feature not in used_features]  # Exclude target column

    if candidate_features:
        for feature in candidate_features:
            gain = info_gain(dataset, feature)
            if gain > max_info_gain:
                max_info_gain = gain
                best_feature = feature

        if best_feature:
            used_features.append(best_feature)
            node.feature = best_feature
            node.possible_values = dataset[best_feature].unique()
            return node
        else:
            return None

    return None

# Recursive function to build the decision tree (ID3 algorithm)
def build_decision_tree(dataset, used_features):
    root = select_best_feature(dataset, used_features)
    decision_tree = {}

    # If root is found, continue building the tree
    if root is not None:
        branches = []
        for value in root.possible_values:
            subset = dataset[dataset[root.feature] == value]
            if calc_entropy(subset) == 0:  # If pure subset, it's a decision (leaf node)
                branches.append((value, subset.iloc[:, -1].unique()[0]))
            else:
                subtree = build_decision_tree(subset, used_features)
                branches.append((value, subtree))

        decision_tree[root.feature] = branches

    return decision_tree

# Function to print the ID3 decision tree with attributes and its decisions
def display_decision_tree(tree, depth=0):
    if isinstance(tree, dict):
        for key, value in tree.items():
            print(f"{'|   ' * depth}{key}")  # Print the feature name
            if isinstance(value, list):
                for val in value:
                    print(f"{'|   ' * (depth + 1)}{val[0]} ->", end=" ")
                    if isinstance(val[1], dict):
                        print()
                        display_decision_tree(val[1], depth + 2)  # Recursive call for further splits
                    else:
                        print(f"Decision: {val[1]}")  # Print the decision
    else:
        print(tree)

# Build the ID3 decision tree
used_features = []
id3_tree = build_decision_tree(dataset, used_features)

# Print the ID3 decision tree with features and decisions
print("Final ID3 Decision Tree:")
display_decision_tree(id3_tree)
